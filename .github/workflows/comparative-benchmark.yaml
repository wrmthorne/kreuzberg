name: Comparative Framework Benchmark

on:
  workflow_dispatch:
    inputs:
      iterations:
        description: "Number of iterations per test"
        required: false
        default: "3"
        type: string
      timeout:
        description: "Timeout per file in seconds"
        required: false
        default: "300"
        type: string

jobs:
  comparative-benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 360

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true
          cache-dependency-glob: |
            uv.lock
            benchmarks/uv.lock

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"

      - name: Cache uv dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            .venv
            benchmarks/.venv
          key: uv-${{ runner.os }}-${{ hashFiles('uv.lock', 'benchmarks/uv.lock') }}
          restore-keys: |
            uv-${{ runner.os }}-

      - name: Cache APT packages
        uses: awalsh128/cache-apt-pkgs-action@latest
        with:
          packages: tesseract-ocr tesseract-ocr-eng tesseract-ocr-deu tesseract-ocr-fra tesseract-ocr-heb tesseract-ocr-chi-sim tesseract-ocr-jpn tesseract-ocr-kor poppler-utils libmagic1 pandoc
          version: 2.0

      - name: Install Python Dependencies
        uses: nick-fields/retry@v3
        with:
          timeout_minutes: 5
          max_attempts: 3
          retry_wait_seconds: 30
          command: |
            echo "Installing entire workspace with all packages and extras..."
            # Install everything for benchmarks - all extras, all packages, all groups
            uv sync --all-extras --all-packages --all-groups
            uv pip install maturin
            uv run maturin develop --release --features extension-module
          shell: bash

      - name: Clear Kreuzberg Cache
        run: |
          echo "Clearing any existing Kreuzberg cache..."
          rm -rf ~/.kreuzberg .kreuzberg benchmarks/.kreuzberg
          echo "Cache cleared"

      - name: Run Benchmarks
        id: benchmark
        run: |
          cd benchmarks

          ITERATIONS="${{ github.event.inputs.iterations }}"
          TIMEOUT="${{ github.event.inputs.timeout }}"

          echo "=== Starting Benchmark Suite ==="
          echo "Iterations: $ITERATIONS"
          echo "Timeout: ${TIMEOUT}s per file"
          echo "Running all frameworks and categories"

          # Run benchmarks and generate aggregated JSON directly
          uv run python -m src.cli \
            --iterations "$ITERATIONS" \
            --timeout "$TIMEOUT" \
            --output aggregated-results/aggregated_results.json

          echo "Benchmark execution completed"

      - name: Generate Visualizations
        if: always()
        run: |
          echo "=== Generating Visualizations ==="
          mkdir -p docs/benchmarks/charts

          if [ -f "benchmarks/aggregated-results/aggregated_results.json" ]; then
            uv run scripts/generate_benchmark_visualizations.py \
              --input benchmarks/aggregated-results/aggregated_results.json \
              --output docs/benchmarks/charts

            echo "âœ… Visualizations generated successfully"
          else
            echo "âš ï¸ No results for visualization"
          fi

      - name: Generate Documentation
        if: always()
        run: |
          echo "=== Generating Benchmark Documentation ==="

          if [ -f "benchmarks/aggregated-results/aggregated_results.json" ]; then
            uv run scripts/generate_benchmark_docs.py \
              --input benchmarks/aggregated-results/aggregated_results.json \
              --output docs/benchmarks \
              --charts docs/benchmarks/charts

            echo "âœ… Documentation generated successfully"
          else
            echo "âš ï¸ No results for documentation"
          fi

      - name: Create Summary
        if: always()
        run: |
          echo "## ðŸ“Š Comparative Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### Configuration" >> $GITHUB_STEP_SUMMARY
          echo "- **Iterations**: ${{ github.event.inputs.iterations }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Timeout**: ${{ github.event.inputs.timeout }}s" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Add summary from aggregated results if available
          if [ -f "benchmarks/aggregated-results/aggregated_results.json" ]; then
            echo "### Results Summary" >> $GITHUB_STEP_SUMMARY
            python -c "
import json
import sys

with open('benchmarks/aggregated-results/aggregated_results.json', 'r') as f:
    data = json.load(f)

if 'error' not in data:
    print(f'- **Total Files Processed**: {data.get(\"total_files_processed\", 0)}')
    print(f'- **Total Runs**: {data.get(\"total_runs\", 0)}')
    print(f'- **Total Time**: {data.get(\"total_time_seconds\", 0):.2f}s')

    if 'framework_summaries' in data:
        print('')
        print('### Framework Performance')
        for fw, summary in data['framework_summaries'].items():
            print(f'**{fw}**:')
            print(f'  - Success Rate: {summary.get(\"success_rate\", 0)*100:.1f}%')
            print(f'  - Avg Time: {summary.get(\"avg_extraction_time\", 0):.2f}s')
            print(f'  - Avg Memory: {summary.get(\"avg_memory_mb\", 0):.1f}MB')
else:
    print('âŒ ' + data.get('error', 'Unknown error'))
" >> $GITHUB_STEP_SUMMARY || echo "âš ï¸ Could not parse results" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ No results file found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload Aggregated Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.run_id }}
          path: benchmarks/aggregated-results/
          retention-days: 90

      - name: Upload Documentation
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-docs-${{ github.run_id }}
          path: docs/benchmarks/
          retention-days: 90
